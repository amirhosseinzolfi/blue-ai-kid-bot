#!/usr/bin/env python
import sys
from langchain_ollama import OllamaEmbeddings
from langchain_openai import ChatOpenAI

# Import LangGraph and LangMem components
from langgraph.store.memory import InMemoryStore
from langmem import create_memory_store_manager, create_manage_memory_tool, create_search_memory_tool
from langgraph.prebuilt import create_react_agent

# ---------------------------
# 1. Initialize LLM and Embeddings
# ---------------------------
llm = ChatOpenAI(
    base_url="http://localhost:15205/v1",
    model_name="gemini-1.5-flash",
    temperature=0.5,
    streaming=True,
    api_key="324"
)

# Initialize Ollama embeddings (assuming output vectors are 768-dimensional)
embeddings = OllamaEmbeddings(model="nomic-embed-text")

# ---------------------------
# 2. Set Up the Long-Term Memory Store
# ---------------------------
store = InMemoryStore(index={
    "dims": 768,      # Dimensionality for the embedding vectors
    "embed": embeddings
})

# ---------------------------
# 3. Create a Memory Manager for Semantic Extraction
# ---------------------------
memory_manager = create_memory_store_manager(
    llm,
    namespace=("agent_memories",),  # Namespace for stored memories
    instructions="Extract and remember key facts, preferences, or context details mentioned in the conversation.",
    enable_inserts=True,
    enable_deletes=True
)

# ---------------------------
# 4. Set Up Memory Tools for the Agent
# ---------------------------
manage_memory_tool = create_manage_memory_tool(namespace=("agent_memories",))
search_memory_tool = create_search_memory_tool(namespace=("agent_memories",))

# ---------------------------
# 5. Create the LangGraph Agent with Integrated Memory
# ---------------------------
agent = create_react_agent(
    llm,
    tools=[manage_memory_tool, search_memory_tool],
    store=store  # Attach the long-term memory store to the agent
)

# ---------------------------
# 6. Terminal Chat Loop with Memory Extraction
# ---------------------------
def run_chat():
    print("=== LangGraph Long-Term Memory Chatbot ===")
    print("Type 'exit' or 'quit' to end the session.\n")
    
    # Maintain conversation history as a list of message dicts.
    conversation_history = []
    
    while True:
        try:
            user_input = input("User: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nExiting chat.")
            break
        
        if user_input.lower() in {"exit", "quit"}:
            print("Exiting chat.")
            break
        
        # Append the user message to conversation history.
        conversation_history.append({"role": "user", "content": user_input})
        
        # Invoke the agent with the current conversation.
        response = agent.invoke({"messages": conversation_history})
        
        # Use dictionary indexing to access the assistant's reply.
        agent_reply = response["messages"][-1].content
        print("Agent:", agent_reply)
        
        # Append the agent's reply to the conversation history.
        conversation_history.append({"role": "assistant", "content": agent_reply})
        
        # Use the memory manager on the last two messages to extract semantic details.
        memory_input = {"messages": conversation_history[-2:]}
        memory_manager.invoke(memory_input)

if __name__ == "__main__":
    run_chat()
